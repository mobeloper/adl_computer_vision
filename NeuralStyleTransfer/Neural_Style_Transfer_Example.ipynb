{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VLWuyTPSMkW"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel 'ipykernel_py3 (Python 3.12.2)'. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. WebSocket is not defined"
          ]
        }
      ],
      "source": [
        "#NST (Neural Style Transfer) - Its all about applying style of style Image to content Image\n",
        "#\n",
        "# Implementation:\n",
        "#=============================================================================================================================\n",
        "# 1. Load and preprocess image\n",
        "# 2. Extract features (FeatureMaps) from both content image and style image (VGG19 etc)\n",
        "# 3. Computing Gram Matrics for Style (Gram Matrics is responsible to capture stylistic Patterns)\n",
        "#\n",
        "#         A Gram Matrix capture correlation between feature maps of layers. (stylistic features)\n",
        "#\n",
        "#             G = F X F(transpose)\n",
        "#\n",
        "#         where,\n",
        "#          F is the reshaped feature map of shape (channels,Height*Width)\n",
        "#\n",
        "# 4. Initialize Target Image (Initialize with Content image or Noise) --- In my example I initialized with Content image\n",
        "#\n",
        "# 5. Define loss function\n",
        "#\n",
        "#      Content Loss - MSE\n",
        "#      Style Loss - MSE\n",
        "#\n",
        "#       Total Loss = alpha * (Content Loss) + beta * (Style Loss)\n",
        "#\n",
        "# Where,\n",
        "#          alpha and beta are Hyperparameters (value range 0 to 1) ---- (Analogy: Similar to Learning Rate)\n",
        "#\n",
        "# 6. optimize the generated target image\n",
        "#\n",
        "# 7. Post Process image ( Convert image from tensor to np array to visualize or save image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6STufChrYQY_"
      },
      "outputs": [],
      "source": [
        "#Applications of NST\n",
        "# 1. Art and Design ----> Generating art work\n",
        "# 2. Gaming Industry ---> Action figure camoflauging in Background\n",
        "# 3. NFTs\n",
        "# 4. Social Media Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpVrlSFYc4ls"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9TTz7lpdCQY"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkkt17HudEKC"
      },
      "outputs": [],
      "source": [
        "# Helper function to load and preprocess images\n",
        "def load_image(image_path, max_size=400, shape=None):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    # Define image loader transformation\n",
        "    if shape is not None:\n",
        "        # Ensure shape is passed as (height, width) for transforms.Resize\n",
        "        loader = transforms.Compose([\n",
        "            transforms.Resize((shape[1], shape[0])),  # (height, width)\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        # If no shape is specified, resize maintaining aspect ratio\n",
        "        loader = transforms.Compose([\n",
        "            transforms.Resize(max_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    image = loader(image).unsqueeze(0)  # Add batch dimension\n",
        "    return image.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1ylzNPldHYR"
      },
      "outputs": [],
      "source": [
        "# Load content image\n",
        "content = load_image(\"old_apartment.jpg\")\n",
        "\n",
        "# Load style image with matching dimensions (width, height)\n",
        "style = load_image(\"lux_apartment.jpg\", shape=(content.shape[-1], content.shape[-2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YRnreV6dKO7"
      },
      "outputs": [],
      "source": [
        "# Helper function to convert tensor to image\n",
        "def im_convert(tensor):\n",
        "    image = tensor.clone().detach().cpu().squeeze(0)\n",
        "    image = image.numpy().transpose(1, 2, 0)\n",
        "    image = image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]  # denormalize\n",
        "    image = image.clip(0, 1)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "tu9KcYWieiIe",
        "outputId": "6a3a281f-90d5-4671-c127-72ff3f4b5410"
      },
      "outputs": [],
      "source": [
        "# Display images\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax[0].imshow(im_convert(content))\n",
        "ax[0].set_title(\"Content Image\")\n",
        "ax[1].imshow(im_convert(style))\n",
        "ax[1].set_title(\"Style Image\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJksjBfgej4o"
      },
      "outputs": [],
      "source": [
        "# Define VGG network\n",
        "class VGGFeatures(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGGFeatures, self).__init__()\n",
        "        self.selected_layers = ['0', '5', '10', '19', '28']  # Conv layers from VGG19\n",
        "        self.vgg = models.vgg19(pretrained=True).features[:29]\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = []\n",
        "        for name, layer in self.vgg._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in self.selected_layers:\n",
        "                features.append(x)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ol6WexQ-emem",
        "outputId": "2724fc89-6566-470b-b384-ebe20bb9da9e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize VGG model\n",
        "vgg = VGGFeatures().to(device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQnIwrWKeoE3"
      },
      "outputs": [],
      "source": [
        "# Function to compute Gram matrix for style\n",
        "\n",
        "# 3. Computing Gram Matrics for Style (Gram Matrics is responsible to capture stylistic Patterns)\n",
        "#\n",
        "#         A Gram Matrix capture correlation between feature maps of layers. (stylistic features)\n",
        "#\n",
        "#             G = F X F(transpose)\n",
        "#\n",
        "#         where,\n",
        "#          F is the reshaped feature map of shape (channels,Height*Width)\n",
        "\n",
        "def gram_matrix(tensor):\n",
        "    _, n_filters, h, w = tensor.size()\n",
        "    tensor = tensor.view(n_filters, h * w)\n",
        "    gram = torch.mm(tensor, tensor.t())\n",
        "    return gram\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEqR159NevAn"
      },
      "outputs": [],
      "source": [
        "# Get style features (detached from graph)\n",
        "style_features = vgg(style)\n",
        "style_grams = [gram_matrix(feat).detach() for feat in style_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ2SYa9veyRm"
      },
      "outputs": [],
      "source": [
        "# Get content features (detached from graph)\n",
        "content_features = [feat.detach() for feat in vgg(content)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCli7v1vez3z"
      },
      "outputs": [],
      "source": [
        "# Initialize target image to optimize (clone content)\n",
        "target = content.clone().requires_grad_(True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcPwzUxpe0yO"
      },
      "outputs": [],
      "source": [
        "# Define weights for style layers\n",
        "style_weights = [1e3 / n**2 for n in [64, 128, 256, 512, 512]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RptGnf97e2gA"
      },
      "outputs": [],
      "source": [
        "# Optimizer\n",
        "optimizer = optim.LBFGS([target])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GspMlyrIe31B"
      },
      "outputs": [],
      "source": [
        "# Style and Content weights\n",
        "alpha = 1e5  # content weight\n",
        "beta = 1e10  # style weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9UuIjRye_qH",
        "outputId": "dd0e55ff-6279-4d9d-e935-051fa46e3e9c"
      },
      "outputs": [],
      "source": [
        "# Optimization Loop\n",
        "epochs = 1000\n",
        "run = [0]\n",
        "\n",
        "while run[0] <= epochs:\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        target_features = vgg(target)\n",
        "\n",
        "        # Compute content loss\n",
        "        content_loss = torch.mean((target_features[2] - content_features[2])**2)\n",
        "\n",
        "        # Compute style loss\n",
        "        style_loss = 0\n",
        "        for t_feat, s_gram, weight in zip(target_features, style_grams, style_weights):\n",
        "            t_gram = gram_matrix(t_feat)\n",
        "            style_loss += weight * torch.mean((t_gram - s_gram)**2)\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = alpha * content_loss + beta * style_loss\n",
        "\n",
        "        # Backward pass (no retain_graph needed)\n",
        "        total_loss.backward()\n",
        "\n",
        "        run[0] += 1\n",
        "        if run[0] % 50 == 0:\n",
        "            print(f\"Epoch {run[0]}, Total Loss: {total_loss.item():.2f}\")\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    optimizer.step(closure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "sWFBhxt6fEh6",
        "outputId": "dd48d944-14d4-4711-87dc-03cfe05d4dfb"
      },
      "outputs": [],
      "source": [
        "# Display final stylized image\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(im_convert(target))\n",
        "plt.title(\"Stylized Image\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ipykernel_py3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
