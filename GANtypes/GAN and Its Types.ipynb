{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMPNHgSXRaJEJNHTpjSzm6y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0ms1PPDZgqJV"},"outputs":[],"source":["# 1. Basic GAN (Vanilla GAN) -------------> Original Idea of GAN proposed by Ian Goodfellow\n","# 2. DCGAN (Deep Convolutional GAN)\n","#       - Uses CNN instead of ANN\n","#       - Better when it comes to creating Images\n","#\n","# 3. WGAN (Wasserstien GAN)\n","#       - Uses Wasserstien Distance formula instead of BCE loss\n","#       - More stable in terms of training\n","#\n","# 4. WGAN with Gradient Penalty (Analogy -- Applying Batch Normalization on all weights calculated)\n","#\n","# 5. Conditional GAN (cGAN)\n","#       - Generates data based on condition of label\n","#\n","# 6. AC-GAN ( Auxillary Classifier GAN)\n","#       - Good for multi-class image generation"]},{"cell_type":"code","source":["# Generator (for MNIST - 28x28 output)\n","class Generator(nn.Module):\n","    def __init__(self, z_dim=100, img_channels=1, feature_g=64):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            #                (inputDim, OutputChannel, kernelSize, stride, padding)\n","            # nn.ConvTranspose2d(100, 256, 3, 1, 0)\n","            nn.ConvTranspose2d(z_dim, feature_g * 4, 3, 1, 0),     # 1x1 → 3x3\n","            nn.BatchNorm2d(feature_g * 4),\n","            nn.ReLU(True),\n","\n","            # nn.ConvTranspose2d(256, 128, 4,2,1)\n","            nn.ConvTranspose2d(feature_g * 4, feature_g * 2, 4, 2, 1),  # 3x3 → 7x7\n","            nn.BatchNorm2d(feature_g * 2),\n","            nn.ReLU(True),\n","\n","             # nn.ConvTranspose2d(128, 64, 4,2,1)\n","            nn.ConvTranspose2d(feature_g * 2, feature_g, 4, 2, 1),      # 7x7 → 14x14   #OutputSize = (7-1)* 2 + 4-2*1 = 14\n","            nn.BatchNorm2d(feature_g),\n","            nn.ReLU(True),\n","             # nn.ConvTranspose2d(64, 1, 4,2,1)\n","            nn.ConvTranspose2d(feature_g, img_channels, 4, 2, 1),       # 14x14 → 28x28\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z):\n","        return self.net(z)\n","\n","# Discriminator (for MNIST - 28x28 input)\n","class Discriminator(nn.Module):\n","    def __init__(self, img_channels=1, feature_d=64):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv2d(img_channels, feature_d, 4, 2, 1),       # 28x28 → 14x14\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(feature_d, feature_d * 2, 4, 2, 1),       # 14x14 → 7x7\n","            nn.BatchNorm2d(feature_d * 2),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(feature_d * 2, 1, 7, 1, 0),               # 7x7 → 1x1\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x).view(-1, 1)"],"metadata":{"id":"w9AdRwdjkCVy"},"execution_count":null,"outputs":[]}]}